{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadísticos Principales\n",
    "\n",
    "## Esperanza\n",
    "La **esperanza** o **valor esperado** o **media** de una v.a. $X$ se denota $E[X]$ o $\\mu$. Es una medida de tendencia central. Se calcula como:\n",
    "\n",
    "$\\begin{array}{ll} \n",
    "E[X] = \n",
    "\\left\\{\\begin{array}{ll} \\sum_x x P(X=x) = \\sum_x x f(x) & si\\,X\\, discreta\\\\\n",
    "                          \\int x f(x)dx & si\\,X\\, continua\\\\\n",
    "\\end{array} \\right .\\\\\n",
    "\\end{array}$\n",
    "\n",
    "\n",
    "Consideremos $g$ una función a valores reales, entonces:\n",
    "\n",
    "$\\begin{array}{lll}\n",
    "E[g(X)] & = & \n",
    "\\left\\{\\begin{array}{ll} \\sum_x g(x) P(X=x) = \\sum_x g(x) f(x) & si\\,X\\, discreta\\\\\n",
    "                          \\int g(x) f(x)dx & si\\,X\\, continua\\\\\n",
    "\\end{array}\\right .\\\\\n",
    "\\end{array}$\n",
    "\n",
    "Para el caso especial de $g(x) = x^n$ se define el n-ésimo **momento** de la v.a. X (o su distribución de probabilidad) como:\n",
    "\n",
    "$\\begin{array}{lll} \n",
    "E[X^n] & = & \n",
    "\\left\\{\\begin{array}{ll} \\sum_x x^n P(X=x) = \\sum_x x^n f(x) & si\\,X\\, discreta\\\\\n",
    "                          \\int x^n f(x)dx & si\\,X\\, continua\\\\\n",
    "\\end{array}\\right .\\\\\n",
    "\\end{array}$\n",
    "\n",
    "- Los momentos son ciertas medidas cuantitativas relacionadas con la forma del gráfico de la función de la distribución de probabilidad. \n",
    "- La esperanza es el primer momento y se denota $\\mu$.\n",
    "- El $n$-ésimo **momento centrado** sobre la media es $\\mu_n = E[(X − \\mu)^n]$. \n",
    "- El $n$-ésimo **momento estándar** es $\\frac{\\mu_n}{\\sigma_n} = \\frac{E[(X − \\mu)^n]}{\\sigma^n} = E[(\\frac{X − \\mu}{\\sigma})^n]$ ($\\sigma$ es la desviación estándar).\n",
    "\n",
    "Para dos variables aleatorias X, Y y una función de ellos $g$:\n",
    "\n",
    "$\\begin{array}{lll} \n",
    "E[g(X, Y)] & = \n",
    "\\left\\{\\begin{array}{ll} \\sum_x \\sum_y g(x, y) P(X=x, Y=y) = \\sum_x \\sum_y g(x, y) f(x, y) & si\\,X,Y\\, discreta\\\\\n",
    "                          \\int \\int g(x, y) f(x, y)dx dy & si\\,X,Y\\, continua\\\\\n",
    "\\end{array}\\right .\\\\\n",
    "\\end{array}$\n",
    "\n",
    "\n",
    "**Propiedades**\n",
    "\n",
    "Sean $a,b \\in \\cal{R}$ entonces:\n",
    "\n",
    "$\\begin{array}{lll} \n",
    "E[aX+b] & = & aE[X] + b \\\\\n",
    "E[X + Y] & = & E[X] + E[Y]\\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varianza, covarianza y correlación\n",
    "### Varianza\n",
    "La **varianza** ($\\sigma^2$) mide la variación de la v.a. en torno a la esperanza o media ($E[X]$ o $\\mu$). Es una medida de dispersión. Se define como\n",
    "\n",
    "$\\begin{array}{lll}\n",
    "Var(X) = E[(X-\\mu)^2] & = & \n",
    "\\left\\{\\begin{array}{ll} \\sum_x (x-\\mu)^2 f(x) & si\\,X\\, discreta\\\\\n",
    "                          \\int (x-\\mu)^2 f(x)dx & si\\,X\\, continua\\\\\n",
    "\\end{array}\\right. = E[X^2] - \\mu^2\\\\\n",
    "\\end{array}$\n",
    "\n",
    "La varianza es el segundo momento centrado sobre la media.\n",
    "\n",
    "**Propiedad**\n",
    "\n",
    "Sean $a,b \\in \\cal{R}$ entonces:\n",
    "\n",
    "$\\begin{array}{ll} \n",
    "Var(aX+b) = a^2 Var(X)\n",
    "\\end{array}$\n",
    "\n",
    "### Desviación estándar\n",
    "Se define además la **desviación estándar** $\\sigma = \\sqrt{Var(X)}$ (mantiene la unidad de X) \n",
    "\n",
    "### Covarianza\n",
    "La **covarianza** mide la relación (lineal) que hay entre dos v.a. $X$ e $Y$. Si denotamos $\\mu_X = E[X]$ y $ \\mu_Y= E[Y]$ entonces:\n",
    "\n",
    "$\\begin{array}{lll}\n",
    "Cov(X,Y) & = & E[(X-\\mu_X)(Y-\\mu_y)] \\\\ \n",
    "& = & \\left\\{\\begin{array}{ll} \\sum_x \\sum_y (x-\\mu_X)(y-\\mu_y) f(x, y) & si\\,X,Y\\, discreta\\\\\n",
    "                          \\int \\int (x-\\mu_X)(y-\\mu_y) f(x, y)dx dy & si\\,X,Y\\, continua\\\\\n",
    "\\end{array}\\right .\\\\\n",
    "\\end{array}$\n",
    "\n",
    "**Propiedades**\n",
    "\n",
    "$\\begin{array}{lll} \n",
    "Cov(X,Y) & = & E[XY] - \\mu_X \\mu_Y \\\\\n",
    "Cov(X,Y) & = & Cov(Y,X) \\\\\n",
    "Cov(X,X) & = & Var(X)\\\\\n",
    "Cov(X+Z,Y) & = & Cov(X,Y) + Cov(Z,Y)\\\\\n",
    "Cov(\\sum_i \\limits X_i,Y) & = & \\sum_i \\limits Cov(X_i,Y)\\\\\n",
    "Var(X+Y) & = & Var(X) + Var(Y) + 2Cov(X,Y)\\\\\n",
    "Var(\\sum_i \\limits X_i) & = & \n",
    "\\sum_i \\limits Var(X_i) + \\sum_i \\limits \\sum_{j\\neq i} \\limits Cov(X_i,X_j)\n",
    "\\end{array}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corelación\n",
    "La **correlación** es una medida normalizada de la covarianza que tiene valor entre -1 y 1:\n",
    "\n",
    "$\\begin{array}{lll}\n",
    "\\rho(X,Y) & = & \\frac{Cov(X,Y)}{\\sigma_x(X) \\sigma_y(Y)}\n",
    "\\end{array}$\n",
    "\n",
    "Lo anterior define la **correlación de Pearson** que mide correlación lineal  entre variables. Cuano se aplica a una población, generalmente usamos $\\rho$ para denotarla; cuando se aplica a un muestra, generalmente usamos $r$ para denotarla. La siguiente figura muestra algunos ejemoplos para yudar a entender cómo mide la correlación lineal. Hay otros tipos de correlación que pueden medir la correlación no lineal ([Spearman](https://es.wikipedia.org/wiki/Coeficiente_de_correlaci%C3%B3n_de_Spearman), [Kendall](https://es.wikipedia.org/wiki/Coeficiente_de_correlaci%C3%B3n_de_rango_de_Kendall)).\n",
    "\n",
    "<img src=\"PearsonCorrelation.png\" width=\"600\">\n",
    "\n",
    "Figura: Ejemoplos para la correlación de Pearson ([fuente](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/10%3A_Correlation_and_Regression/10.02%3A_The_Linear_Correlation_Coefficient))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corelación y Independencia**\n",
    "\n",
    "En la última imagen, $r = 0$, pero los dos v.a. tienen una relación no lineal y no son independentes. Por ejemplo, $P(X=x)=\\frac{1}{3}$ para $X$=-1, 0, o 1, y $Y=-X^2$.\n",
    "\n",
    "Cuál es la relación entre zero corelación y independencia? \n",
    "\n",
    "- Si $X$, $Y$ son independientes, $\\quad$ entonces $Cov(X,Y) = 0$ o $\\rho = 0$. (Por qué?)\n",
    "- Si $Cov(X,Y) = 0$ o $\\rho = 0$, $\\quad$ $X$, $Y$ no son necesariamente independientes.\n",
    "\n",
    "Cuál es el diagrama de Venn de ellos?\n",
    "\n",
    "### Propiedades en el caso de independencia\n",
    "\n",
    "Si $X$, $Y$ son v.a. independientes, entonces:\n",
    "\n",
    "$\\begin{array}{lll} \n",
    "E[XY] & = & E[X]E[Y] \\\\\n",
    "Cov(X,Y) & = & 0\\\\\n",
    "Var(X+Y) & = & Var(X) + Var(Y)\\\\\n",
    "\\end{array}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asimetría y curtosis\n",
    "### Asimetría (skewness)\n",
    "Las medidas de asimetría son indicadores que permiten establecer el grado de simetría (o asimetría). La asimetría es el tercer momento estándar de una distribución.\n",
    "\n",
    "$\\begin{array}{lll}\n",
    "\\text{ Asimetría (skewness) } & = & E[(\\frac{X-\\mu}{\\sigma})^3] = \\frac{E[(X-\\mu)^3]}{\\sigma^3} = \\frac{E[X^3]-3\\mu\\sigma^2 - \\mu^3}{\\sigma^3}\\\\\n",
    "\\end{array}$\n",
    "\n",
    "<img src=\"asimetria.png\" width=\"550\">\n",
    "\n",
    "Figura: Asimetría ([fuente](https://www.probabilidadyestadistica.net/asimetria-estadistica/))\n",
    "\n",
    "### Curtosis\n",
    "La curtosis es una medida de que tan gruesa la cola y plano está una distribución, respecto a la distribución normal. La curtosis es el cuarto momento estándar de una distribución.\n",
    "\n",
    "$\\begin{array}{lll}\n",
    "\\text{ Curtosis } & = &E[(\\frac{X-\\mu}{\\sigma})^4] = \\frac{E[(X-\\mu)^4]}{\\sigma^4} = \\frac{E[X^4] - 4\\mu E[X^3] + 6\\mu^2\\sigma^2 + 3\\mu^4}{\\sigma^4}\\\\\n",
    "\\text{ Exceso de curtosis } & = & Curtosis - 3\\\\\n",
    "\\end{array}$\n",
    "\n",
    "<img src=\"curtosis.png\" width=\"450\">\n",
    "\n",
    "Figura: $\\beta_2$ es la medida de curtosis aquí ([fuente](https://www.probabilidadyestadistica.net/asimetria-estadistica/))\n",
    "\n",
    "- Una mayor curtosis implica una mayor concentración de valores lejos del centro (es decir, las colas más gruesas), y a menudo más puntiaguda, respecto a la normal.\n",
    "- Una menor curtosis implica una menor concentración de valores lejos del centro (es decir, las colas menos gruesas), y a menudo más plano, respecto a la normal.\n",
    "\n",
    ":::{warning}\n",
    "Hay varios conceptos erróneos comunes sobre la curtosis en materiales en línea y libros de texto. Aquí están los entendimientos correctos: \n",
    "- La curtosis **no solo implica que tan puntiaguda** está la distribución, también implica que tan gruesa la cola principalmente.\n",
    "- Una mayor curtosis implica colas **más gruesas** no menos gruesa, y a menudo más puntiaguda, respecto a la normal. \n",
    "- Una mayor curtosis **no implica una mayor varianza**, ni viceversa. La curtosis se escala con respecto a la varianza, por lo que no se ve afectada por ella.\n",
    "\n",
    "Más detalles se pueden encontrar [aquí](http://www.columbia.edu/~ld208/psymeth97.pdf).\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediana y moda\n",
    "La **mediana** es el valor de la variable de posición central en un conjunto de datos ordenados. \n",
    "\n",
    "La **moda** es el valor que aparece con mayor **frecuencia** en un conjunto de datos.\n",
    "\n",
    "Ambas son otras medidas de tendencia central.\n",
    "\n",
    "<img src=\"mediana_moda.png\" width=\"550\">\n",
    "\n",
    "Figura: Mediana, moda y media de diferentes distribuciones ([fuente](https://epamatematicas.blogspot.com/2019/08/relacion-entre-la-media-la-mediana-y-la.html))\n",
    "\n",
    ":::{warning}\n",
    "De menudo se usa la media para informar la tendencia central. Pero no es una estadística robusta: está muy influida por los valores atípicos (outliers). En las distribuciones asimétricas, la mediana proporciona una mejor descripción de la tendencia central.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "jupyter-book-info337",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "python",
   "pygments_lexer": "r",
   "version": "3.9.16 (main, May 15 2023, 18:51:40) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "671065b571a5fdb465b5ce5eb6a0c42c0bf0a3bd98adc4932f551a92e7f95554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
