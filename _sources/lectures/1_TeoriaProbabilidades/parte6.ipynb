{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoremas Asintóticos\n",
    "\n",
    "## Leyes de los grandes números\n",
    "\n",
    "**Las leyes de los grandes números** dicen que la media de una muestra grande está cerca de la media de la distribución de la población. Estas leyes son importantes porque garantizan resultados estables a largo plazo para las medias de algunos eventos aleatorios. \n",
    "\n",
    "Aquí veamos la **ley débil de los grandes números** primero. \n",
    "\n",
    "Sean $X_1,...X_n$ v.a. independientes idénticamente distribuidas (i.i.d.) de media $\\mu$. Entonces, $\\overline{X}_n=\\frac{1}{n}\\sum_{i=1}^n X_i$ cumple:\n",
    "\n",
    "$$\\lim_{n \\to \\infty} P\\left\\{\\middle | \\overline{X}_n - \\mu \\,\\middle | < \\epsilon \\right\\} = 1$$\n",
    "\n",
    "para cualquier $\\epsilon >0$.\n",
    "\n",
    "¿Cómo se entiende esto? \n",
    "\n",
    "```{toggle}\n",
    "Usualmente se escribe como \n",
    "\n",
    "$$\\begin{array}{c} \n",
    "\\overline{X}_n \\overset{p}{\\to} \\mu \\\\\n",
    "(\\overline{X}_n \\text{ converge en probabilidad a X cuando } n \\to \\infty)\n",
    "\\end{array}$$\n",
    "\n",
    "Es decir, la distribución de la media muestral $\\overline{X}_n$ se concentra más alrededor de la verdadera media $\\mu$ a medida que $n$ se hace grande.\n",
    "```\n",
    "\n",
    "Ahora, ¿puedes aplicar esta ley para explicar por qué podemos obtener la probabilidad utilizando la frecuencia? \n",
    "- Sea $n$ el número de experimentos, y $n(A)$ el número de veces que el evento $A$ ocurre en la realización de esos experimentos, entonces $P(A) = \\frac{n(A)}{n}$\n",
    "\n",
    "(Pista: en la ley arriba, cada $X_i$ puede verse como una variable Bernoulli que indica si el evento $A$ occure o no). \n",
    "\n",
    "En concreto, la explicación corresponde a la [ley fuerte de Borel de los grandes números](https://en.wikipedia.org/wiki/Law_of_large_numbers#Borel's_law_of_large_numbers) que es un caso especial de las leyes más generales de los grandes números. \n",
    "\n",
    "Formalmente, si n(A) es el número de éxitos en los n experimentos Bernoulli repetidos independientes con la probabilidad de exito p, entonces, \n",
    "\n",
    "$$ P(\\lim_{n \\to \\infty} \\frac{n(A)}{n} = p) = 1$$\n",
    "\n",
    "Usualmente se escribe como: \n",
    "$$\\begin{array}{c} \n",
    "\\frac{n(A)}{n} \\overset{c.s.} {\\to} p \\\\\n",
    "(\\frac{n(A)}{n} \\text{ converge casi seguramente, o con probabilidad 1 a p cuando } n \\to \\infty)\n",
    "\\end{array}$$\n",
    "\n",
    "Esta ley vincula el concepto abstracto de probabilidad con la frecuencia, i.e., la proporción de veces que se espera que ocurra un evento (A) determinado es aproximadamente igual a la probabilidad de que ocurra en un ensayo concreto (o la probabilidad real/poblacional). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teorema del Límite Central\n",
    "La ley de los grandes números dice que la distribución de $\\overline{X}_n$ se acumula cerca de $\\mu$. Pero esto no es suficiente para ayudarnos a aproximar la distribución de probabilidad sobre $\\overline{X}_n$. Para ello necesitamos el teorema del límite central.\n",
    "\n",
    "**El teorema del límite central** dice que la media muestral tiene aproximadamente una distribución Normal para una muestra grande. \n",
    "\n",
    "Sean $X_1,...X_n$ v.a. i.i.d. de media $\\mu$ y varianza $\\sigma^2$, entonces $\\overline{X}_n=\\frac{1}{n}\\sum_{i=1}^n X_i$ cumple:\n",
    "\n",
    "$$ \\lim_{n \\to \\infty}P\\left( \\frac{\\overline{X}_n - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} \\leq z\\right) = \\Phi(z)  \\qquad Z \\sim \\cal{N}(0,1)$$\n",
    "\n",
    "¿Cómo se entiende esto? \n",
    "\n",
    "```{toggle}\n",
    "Usualmente se escribe como \n",
    "\n",
    "$$\\begin{array}{c} \n",
    "\\frac{\\overline{X}_n - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\overset{d}{\\to} Z \\\\ \\\\\n",
    "(\\frac{\\overline{X}_n - \\mu}{\\sqrt{\\frac{\\sigma^2}{n}}} \\text{ converge en distribución a Z cuando } n \\to \\infty)\n",
    "\\end{array}$$\n",
    "\n",
    "o \n",
    "\n",
    "$$\\begin{array}{c} \n",
    "\\overline{X}_n \\overset{d}{\\to} \\cal{N}(\\mu,\\frac{\\sigma^2}{n})\n",
    "\\end{array}$$\n",
    "\n",
    "Es decir que es posible aproximar la distribución de $\\overline{X}_n$ por ${\\cal{N}}(\\mu,\\frac{\\sigma^2}{n})$ que tiene la misma media y $\\frac{1}{n}$ de la varianza de la distribución de población.</span>\n",
    "```\n",
    "\n",
    "Esta simulación puede ayudar a entenderlo: [Simulation](https://onlinestatbook.com/stat_sim/sampling_dist/)\n",
    "- Ojo: hay un error en esta simulación, puedes identificalo? \n",
    "- Esta [video](https://onlinestatbook.com/2/sampling_distributions/clt_demo.html#video) (en la derecha de la página) demostra una simulación más correcta. \n",
    "\n",
    ":::{note}\n",
    "- En este teorema, $X_i$ no tiene que tener una distribución normal; podría ser cualquier distribución.\n",
    "- El teorema del límite central es sobre la media muestral, no sobre la variable aleatoria en sí misma.\n",
    ":::\n",
    "\n",
    "Una aplicación muy importante de este teorema consiste en determinar valores razonables de la media de la población $\\mu$. Temas como prueba de hipótesis, estimación, y muchos otros utilizan este teorema. Vamos a repasar este teorema en alguna clase en el futuro."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "671065b571a5fdb465b5ce5eb6a0c42c0bf0a3bd98adc4932f551a92e7f95554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
