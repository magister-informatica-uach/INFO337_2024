

<!DOCTYPE html>


<html lang="es" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>22. Introducción a Markov Chain Monte Carlo &#8212; Herramientas estadísticas para la investigación</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/11_MCMC/lecture1';</script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" />
    <link rel="next" title="23. Tutorial de MCMC con numpyro" href="lecture2.html" />
    <link rel="prev" title="21. Modelamiento Bayesiano" href="../10_ModelamientoBayesiano/ModelamientoBayesiano.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="es"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Saltar al contenido principal</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Elementos de Teoría de Probabilidades</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_TeoriaProbabilidades/parte1.html">1. Elementos Básicos de Teoría de Probabilidades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_TeoriaProbabilidades/parte2.html">2. Variables Aleatorias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_TeoriaProbabilidades/parte3.html">3. Estadísticos Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_TeoriaProbabilidades/parte4.html">4. Variables Aleatorias Especiales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_TeoriaProbabilidades/parte5.html">5. Estadísticos Muestrales</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_TeoriaProbabilidades/parte6.html">6. Teoremas Asintóticos</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modelamiento Estadístico</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../2_statistical_modeling/intro.html">7. Inferencia con Modelos Paramétricos y No Paramétricos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_statistical_modeling/part1.html">8. Modelamiento Paramétrico con MLE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_statistical_modeling/part2.html">9. Modelamiento con métodos no-paramétricos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_statistical_modeling/part3.html">10. Modelamiento parámetrico Bayesiano</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Pensamiento Estadístico</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_InferenciaParametrica/parte1.html">11. Inferencia en Estadística Paramétrica</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_nonparametric_inference/part1.html">12. Inferencia estadística con tests no-paramétricos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_nonparametric_inference/part2.html">13. <em>Bootstrap</em> no paramétrico</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_ANOVA/anova_y_asociacionDiscretas.html">14. Análisis de Asociación de Variables Aleatorias</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regresión Lineal</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_linear_regression/part1.html">15. Introducción a la Regresión Lineal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_linear_regression/part2.html">16. Regresión Lineal Multivariada</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_linear_regression/part3.html">17. Regresión lineal con funciones base y Regularización</a></li>
<li class="toctree-l1"><a class="reference internal" href="../7_RegresionesRespDiscreta/Regresiones_RespuestaDiscreta.html">18. Regresiones para Respuestas Discretas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Análisis Exploratorio</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../8_PCA/PCA.html">19. Análisis Exploratorio y Reducción de la Dimensionalidad</a></li>
<li class="toctree-l1"><a class="reference internal" href="../9_mixture_models/lecture.html">20. Modelos de Mezcla de Gaussianas</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Modelamiento Bayesiano</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../10_ModelamientoBayesiano/ModelamientoBayesiano.html">21. Modelamiento Bayesiano</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">22. Introducción a Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture2.html">23. Tutorial de MCMC con <code class="docutils literal notranslate"><span class="pre">numpyro</span></code></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/magister-informatica-uach/INFO337" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Repositorio de origen"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Descarga esta pagina">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/11_MCMC/lecture1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Descargar archivo fuente"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Imprimir en PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Modo de pantalla completa"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="claro/oscuro" aria-label="claro/oscuro" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Búsqueda" aria-label="Búsqueda" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introducción a Markov Chain Monte Carlo</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contenido </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resumen-de-la-premisa-bayesiana">22.1. Resumen de la premisa Bayesiana</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integracion-por-metodo-de-monte-carlo">22.2. Integración por método de Monte Carlo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#de-monte-carlo-a-markov-chain-monte-carlo">22.3. De Monte Carlo a Markov Chain Monte Carlo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#programacion-probabilistica-pp">22.4. Programación probabilística (PP)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduccion-a-markov-chain-monte-carlo">
<h1><span class="section-number">22. </span>Introducción a Markov Chain Monte Carlo<a class="headerlink" href="#introduccion-a-markov-chain-monte-carlo" title="Permalink to this heading">#</a></h1>
<section id="resumen-de-la-premisa-bayesiana">
<h2><span class="section-number">22.1. </span>Resumen de la premisa Bayesiana<a class="headerlink" href="#resumen-de-la-premisa-bayesiana" title="Permalink to this heading">#</a></h2>
<p>En inferencia Bayesiana, la probabilidad de un evento representa el grado de conocimiento (o desconocimiento) que manejamos de dicho evento.</p>
<p>Matemáticamente, representamos el evento de interés con una variable aleatoria <span class="math notranslate nohighlight">\(\theta\)</span>, y nuestro conocimiento actual del mismo, es decir su <strong>distribución a priori</strong> con <span class="math notranslate nohighlight">\(p(\theta)\)</span>.</p>
<p>Luego, recolectamos datos <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> y actualizamos lo que sabemos de <span class="math notranslate nohighlight">\(\theta\)</span> formando una <strong>distribución a posteriori</strong> <span class="math notranslate nohighlight">\(p(\theta|\mathcal{D})\)</span>.</p>
<p>Lo anterior se obtiene aplicando el Teorema de Bayes:</p>
<div class="math notranslate nohighlight">
\[
p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})} = \frac{p(\mathcal{D}|\theta)p(\theta)}{\int p(\mathcal{D}, \theta) \, d\theta} = \frac{p(\mathcal{D}|\theta)p(\theta)}{\int p(\mathcal{D}|A\theta)p(\theta) \,d\theta},
\]</div>
<p>donde también hicimos uso de la ley de probabilidad total para escribir la verosimilitud marginal <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> en función de la verosimilitud y el prior.</p>
<p>Anteriormente, hemos visto modelos donde la expresión anterior tiene solución analítica (prior conjugados). Sin embargo, estos modelos suelen ser bastante simples.</p>
<p>Para obtener el posterior en modelos más complejos debemos utilizar técnicas de inferencia aproximada o técnicas basdas en <strong>Monte-Carlo Markov Chain (MCMC)</strong>. Esta última es la que se revisa en esta lección.</p>
</section>
<section id="integracion-por-metodo-de-monte-carlo">
<h2><span class="section-number">22.2. </span>Integración por método de Monte Carlo<a class="headerlink" href="#integracion-por-metodo-de-monte-carlo" title="Permalink to this heading">#</a></h2>
<p>Los métodos de Monte Carlo obtienen resultados numéricas en base a muestreo aleatorio. Una de sus más importantes aplicaciones es la integración por Monte Carlo.</p>
<p>Digamos que tenemos un valor esperado sobre una función <span class="math notranslate nohighlight">\(g\)</span> que se evalua en una variable aleatoria <span class="math notranslate nohighlight">\(X\)</span> cuya distribución es <span class="math notranslate nohighlight">\(p(x)\)</span>, por definición esto es:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[g(X)] = \int g(x) p(x) \,dx
\]</div>
<p>Si esta integral es difícil de calcular y tenemos muestras de <span class="math notranslate nohighlight">\(p(x)\)</span> entonces podemos aproximar el valor esperado como:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[g(X)] \approx \hat g_N = \frac{1}{N} \sum_{i=1}^N g(x_i) \quad x_i \sim p(x)
\]</div>
<p>que debido al teorema central del límite:</p>
<div class="math notranslate nohighlight">
\[
\hat g_N \sim \mathcal{N} \left( \mathbb{E}[g(X)], \sigma_N^2/N \right)
\]</div>
<p>es decir que, mientras más muestras tengamos, más se concentrará nuestro estimador en torno al valor real que estamos buscando.</p>
</section>
<section id="de-monte-carlo-a-markov-chain-monte-carlo">
<h2><span class="section-number">22.3. </span>De Monte Carlo a Markov Chain Monte Carlo<a class="headerlink" href="#de-monte-carlo-a-markov-chain-monte-carlo" title="Permalink to this heading">#</a></h2>
<p>El problema con la integración por Monte Carlo es que no siempre es posible obtener muestras de <span class="math notranslate nohighlight">\(p(x)\)</span>. En otros casos obtener muestras es posible pero muy ineficiente de realizar en la práctica.</p>
<p>Para obtener muestras de distribuciones que sólo podemos evaluar hasta una constante de normalización (por ejemplo un posterior), podemos utilizar Markov Chain Monte Carlo (MCMC).</p>
<p>MCMC es una familia de algoritmos que aprenden la probabilidad de transición de una <em>cadena de markov</em> tal que esta converja a una distribución deseada. Una cadena de markov es un proceso aleatorio <span class="math notranslate nohighlight">\(\{X_n\}_{n=0,1,\ldots}\)</span>, es decir una secuencia de variables aleatorias que cumplen la siguiente propiedad:</p>
<div class="math notranslate nohighlight">
\[
p(X_n|X_{n-1},\ldots, X_0) = p(X_n|X_{n-1})
\]</div>
<div class="admonition important">
<p class="admonition-title">Importante</p>
<p>En una cadena de markov la probabilidad del estado futuro es condicionalmente independiente del pasado si conozco el presente.</p>
</div>
<p>Las cadenas de markov que son irreducibles convergen a una distribución estacionaria. Los métodos de MCMC se basan en esta idea para construir una cadena de markov que converga a la distribución que nos interesa, por ejemplo un posterior bayesiano.</p>
<p>En el siguiente diagrama se muestra un color rojo una distribución compleja de la cual nos interesa obtener muestras. La figura de la izquierda muestra la técnica de muestreo por importancia. La figura de la derecha en cambio muestra como funciona MCMC.</p>
<a class="reference internal image-reference" href="../../_images/is_mcmc.png"><img alt="../../_images/is_mcmc.png" src="../../_images/is_mcmc.png" style="width: 500px;" /></a>
<p>MCMC recorre el espacio de una forma menos ingenua.</p>
<blockquote>
<div><p>La secuencia de muestras de la cadena de Markov la llamamos traza.</p>
</div></blockquote>
<p>La clave en los métodos de MCMC está en como se realizan las transiciones, es decir como escogemos el siguiente punto de la cadena. Existen muchos métodos para generar “propuestas”, siendo los siguientes tal vez los más populares.</p>
<p><strong>Metropolis Hastings (MH)</strong></p>
<p>Caminante aleatorio que se mueve en todas las dimensiones de forma simultanea. En MH los candidatos se muestrean a partir de una distribución simétrica <span class="math notranslate nohighlight">\(x^{new} \sim g(x^{new}|x_t)\)</span>.</p>
<p>Luego el paso se acepta si <span class="math notranslate nohighlight">\(p(x^{new})/p(x^t)\)</span> es mayor o igual a un umbral.</p>
<p>Dado que lo anterior es cociente, sólo necesitamos conocer <span class="math notranslate nohighlight">\(f(\cdot)\)</span> hasta una constante. Por ejemplo si dividimos dos posteriors:</p>
<div class="math notranslate nohighlight">
\[
\frac{p(\theta^{new}|\mathcal{D})}{p(\theta^t|\mathcal{D})} = \frac{p(\mathcal{D}|\theta^{new})p(\theta^{new})}{p(\mathcal{D}|\theta^t)p(\theta^t)}
\]</div>
<blockquote>
<div><p>La evidencia (denominador del posterior) se cancela.</p>
</div></blockquote>
<p><strong>Monte-Carlo Hamiltoniano (HMC)</strong></p>
<p>Familia de métodos que realizan propuestas basado en gradiantes, por ende sólo pueden utilizarse para distribuciones de parámetros continuos.</p>
<p>En comparación a MH, cada paso de HMC cuesta más en términos computacionales pero avanza mucho más rápido, es decir requiere menos pasos que MH.</p>
<div class="admonition seealso">
<p class="admonition-title">Ver también</p>
<p>Revise el siguiente sitio web donde se ejemplifica como HMC utiliza la geometría del espacio para realizar mejores propuestas que MH: <a class="reference external" href="http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html</a></p>
</div>
</section>
<section id="programacion-probabilistica-pp">
<h2><span class="section-number">22.4. </span>Programación probabilística (PP)<a class="headerlink" href="#programacion-probabilistica-pp" title="Permalink to this heading">#</a></h2>
<p>Extracto de <a class="reference external" href="https://probabilistic-programming.org/:">https://probabilistic-programming.org/:</a></p>
<blockquote>
<div><p>Probabilistic programming languages aim to unify general purpose programming with probabilistic modeling. The user specifies the probabilistic model (priors, likelihood, etc) and inference follows automatically given the specification</p>
</div></blockquote>
<p>Librerías y <em>frameworks</em> de lenguaje Python para hacer PP y que incorporan métodos de MCMC:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.pymc.io/notebooks/getting_started.html">PyMC3</a></p></li>
<li><p><a class="reference external" href="https://pystan.readthedocs.io/en/latest/">PyStan</a></p></li>
<li><p><a class="reference external" href="http://dfm.io/emcee/current/">emcee</a></p></li>
<li><p><a class="reference external" href="http://edwardlib.org/">Edward</a></p></li>
<li><p><a class="reference external" href="http://pyro.ai">Pyro</a></p></li>
<li><p><a class="reference external" href="http://num.pyro.ai/en/stable/">NumPyro</a></p></li>
</ul>
<p>El siguiente diagrama contrasta la programación tradicional con PP:</p>
<p><a href="https://arxiv.org/abs/1809.10756"><img alt="../../_images/PP.png" src="../../_images/PP.png" /></a></p>
<blockquote>
<div><p>PP avanza en ambas direcciones.</p>
</div></blockquote>
<div class="admonition seealso">
<p class="admonition-title">Ver también</p>
<ul class="simple">
<li><p>Davidson-Pilon, “<a class="reference external" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">Bayesian methods for hackers</a>”, <em>Addison Wesley</em>, 2016, <strong>Capítulos 2 y 3</strong></p></li>
<li><p>Jan-Willem van de Meent et al. “<a class="reference external" href="https://arxiv.org/abs/1809.10756">An Introduction to Probabilistic Programming</a>”</p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/11_MCMC"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../10_ModelamientoBayesiano/ModelamientoBayesiano.html"
       title="página anterior">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">anterior</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Modelamiento Bayesiano</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture2.html"
       title="siguiente página">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">siguiente</p>
        <p class="prev-next-title"><span class="section-number">23. </span>Tutorial de MCMC con <code class="docutils literal notranslate"><span class="pre">numpyro</span></code></p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contenido
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resumen-de-la-premisa-bayesiana">22.1. Resumen de la premisa Bayesiana</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integracion-por-metodo-de-monte-carlo">22.2. Integración por método de Monte Carlo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#de-monte-carlo-a-markov-chain-monte-carlo">22.3. De Monte Carlo a Markov Chain Monte Carlo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#programacion-probabilistica-pp">22.4. Programación probabilística (PP)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
Por Eliana Scheihing, Pablo Huijse y Yun Huang
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>